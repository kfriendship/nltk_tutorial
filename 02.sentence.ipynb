{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br></br>\n",
    "# **Phrase 분석**\n",
    "\n",
    "<br></br>\n",
    "## **1 불용어 처리**\n",
    "Stop Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'i like such a wonderful snow ice cream'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts = 'I like such a Wonderful Snow Ice Cream'\n",
    "texts = texts.lower()\n",
    "texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/yyoo/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# NEED TO RUN ONLY ONCE \n",
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mpunkt\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt')\n  \u001b[0m\n  Searched in:\n    - '/Users/yyoo/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n    - '/Users/yyoo/tf9/nltk_data'\n    - '/Users/yyoo/tf9/share/nltk_data'\n    - '/Users/yyoo/tf9/lib/nltk_data'\n    - ''\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-37161d1d6f14>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mword_tokenize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mword_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtexts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mtokens\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/tf9/lib/python3.6/site-packages/nltk/tokenize/__init__.py\u001b[0m in \u001b[0;36mword_tokenize\u001b[0;34m(text, language, preserve_line)\u001b[0m\n\u001b[1;32m    126\u001b[0m     \u001b[0;34m:\u001b[0m\u001b[0mtype\u001b[0m \u001b[0mpreserver_line\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m     \"\"\"\n\u001b[0;32m--> 128\u001b[0;31m     \u001b[0msentences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mpreserve_line\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0msent_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    129\u001b[0m     return [token for sent in sentences\n\u001b[1;32m    130\u001b[0m             for token in _treebank_word_tokenizer.tokenize(sent)]\n",
      "\u001b[0;32m~/tf9/lib/python3.6/site-packages/nltk/tokenize/__init__.py\u001b[0m in \u001b[0;36msent_tokenize\u001b[0;34m(text, language)\u001b[0m\n\u001b[1;32m     92\u001b[0m     \u001b[0;34m:\u001b[0m\u001b[0mparam\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mPunkt\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m     \"\"\"\n\u001b[0;32m---> 94\u001b[0;31m     \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'tokenizers/punkt/{0}.pickle'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlanguage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/tf9/lib/python3.6/site-packages/nltk/data.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(resource_url, format, cache, verbose, logic_parser, fstruct_reader, encoding)\u001b[0m\n\u001b[1;32m    834\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    835\u001b[0m     \u001b[0;31m# Load the resource.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 836\u001b[0;31m     \u001b[0mopened_resource\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_open\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresource_url\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    837\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    838\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mformat\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'raw'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/tf9/lib/python3.6/site-packages/nltk/data.py\u001b[0m in \u001b[0;36m_open\u001b[0;34m(resource_url)\u001b[0m\n\u001b[1;32m    952\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    953\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mprotocol\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'nltk'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 954\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    955\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'file'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    956\u001b[0m         \u001b[0;31m# urllib might not use mode='rb', so handle this one ourselves:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/tf9/lib/python3.6/site-packages/nltk/data.py\u001b[0m in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    673\u001b[0m     \u001b[0msep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'*'\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m70\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    674\u001b[0m     \u001b[0mresource_not_found\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'\\n%s\\n%s\\n%s\\n'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0msep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 675\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresource_not_found\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    676\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mpunkt\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt')\n  \u001b[0m\n  Searched in:\n    - '/Users/yyoo/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n    - '/Users/yyoo/tf9/nltk_data'\n    - '/Users/yyoo/tf9/share/nltk_data'\n    - '/Users/yyoo/tf9/lib/nltk_data'\n    - ''\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "from nltk import word_tokenize\n",
    "tokens = word_tokenize(texts)\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i', 'her', 'those', 'an', 'into', 'further', 'such', 'now', 'mightn']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stopwords.words('english')[::18]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['like', 'wonderful', 'snow', 'ice', 'cream']\n"
     ]
    }
   ],
   "source": [
    "tokens = [word   for word in tokens   \n",
    "                 if word not in stopwords.words('english')]\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br></br>\n",
    "## **2 한글의 불용어 처리**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'이': 'NP',\n",
       " '있': 'VA',\n",
       " '하': 'VV',\n",
       " '것': 'NNB',\n",
       " '들': 'VV',\n",
       " '그': 'MM',\n",
       " '되': 'VV',\n",
       " '수': 'NNB',\n",
       " '보': 'VX',\n",
       " '않': 'VX',\n",
       " '없': 'VA',\n",
       " '나': 'VX',\n",
       " '사람': 'NNG',\n",
       " '주': 'VV',\n",
       " '아니': 'VCN',\n",
       " '등': 'NNB',\n",
       " '같': 'VA',\n",
       " '우리': 'NP',\n",
       " '때': 'NNG',\n",
       " '년': 'NNB',\n",
       " '가': 'VV',\n",
       " '한': 'MM',\n",
       " '지': 'VX',\n",
       " '대하': 'VV',\n",
       " '오': 'VV',\n",
       " '말': 'VX',\n",
       " '일': 'NNB',\n",
       " '그렇': 'VA',\n",
       " '위하': 'VV',\n",
       " '때문': 'NNB',\n",
       " '그것': 'NP',\n",
       " '두': 'VV',\n",
       " '말하': 'VV',\n",
       " '알': 'VV',\n",
       " '그러나': 'MAJ',\n",
       " '받': 'VV',\n",
       " '못하': 'VX',\n",
       " '그런': 'MM',\n",
       " '또': 'MAG',\n",
       " '문제': 'NNG',\n",
       " '더': 'MAG',\n",
       " '사회': 'NNG',\n",
       " '많': 'VA',\n",
       " '그리고': 'MAJ',\n",
       " '좋': 'VA',\n",
       " '크': 'VA',\n",
       " '따르': 'VV',\n",
       " '중': 'NNB',\n",
       " '나오': 'VV',\n",
       " '가지': 'VV',\n",
       " '씨': 'NNB',\n",
       " '시키': 'XSV',\n",
       " '만들': 'VV',\n",
       " '지금': 'NNG',\n",
       " '생각하': 'VV',\n",
       " '그러': 'VV',\n",
       " '속': 'NNG',\n",
       " '하나': 'NR',\n",
       " '집': 'NNG',\n",
       " '살': 'VV',\n",
       " '모르': 'VV',\n",
       " '적': 'XSN',\n",
       " '월': 'NNB',\n",
       " '데': 'NNB',\n",
       " '자신': 'NNG',\n",
       " '안': 'MAG',\n",
       " '어떤': 'MM',\n",
       " '내': 'VV',\n",
       " '경우': 'NNG',\n",
       " '명': 'NNB',\n",
       " '생각': 'NNG',\n",
       " '시간': 'NNG',\n",
       " '그녀': 'NP',\n",
       " '다시': 'MAG',\n",
       " '이런': 'MM',\n",
       " '앞': 'NNG',\n",
       " '보이': 'VV',\n",
       " '번': 'NNB',\n",
       " '다른': 'MM',\n",
       " '어떻': 'VA',\n",
       " '여자': 'NNG',\n",
       " '개': 'NNB',\n",
       " '전': 'NNG',\n",
       " '사실': 'NNG',\n",
       " '이렇': 'VA',\n",
       " '점': 'NNG',\n",
       " '싶': 'VX',\n",
       " '정도': 'NNG',\n",
       " '좀': 'MAG',\n",
       " '원': 'NNB',\n",
       " '잘': 'MAG',\n",
       " '통하': 'VV',\n",
       " '소리': 'NNG',\n",
       " '놓': 'VX'}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f = open('./data/한국어불용어100.txt', 'r')\n",
    "s = f.read(); f.close()\n",
    "\n",
    "stop_words = [ txt.split('\\t')[:2]  for txt in s.split('\\n') ]\n",
    "stopword   = {}\n",
    "for txt in stop_words:\n",
    "    try:    stopword[txt[0]] = txt[1]\n",
    "    except: pass\n",
    "stopword"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br></br>\n",
    "## **3 Token 객체를 활용한 통계적 분석**\n",
    "1. 레벤슈타인의 편집거리\n",
    "1. Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12, 12, 12)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text1 = \"자연 언어에 대한 연구는 오래전부터 이어져 오고 있음에도 2018년까지도 사람처럼 이해하지는 못한다.\".split()\n",
    "text2 = \"자연 언어에 대한 연구는 오래전부터 이어져 들어서도 아직 컴퓨터가 사람처럼 이해하지는 못한다.\".split()\n",
    "text3 = \"자연 아직 컴퓨터가 언어에 들어서도 못한다 이어져 사람처럼 이해하지는 대한 연구는 오래전부터.\".split()\n",
    "len(text1), len(text2), len(text3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.metrics import edit_distance\n",
    "edit_distance('파이썬 알고리즘', '파파미 알탕')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "생략된 단어가 다를 때 : 3 \n",
      "어휘 순서를 바꿨을 때 : 10\n"
     ]
    }
   ],
   "source": [
    "print('생략된 단어가 다를 때 : {} \\n어휘 순서를 바꿨을 때 : {}'.format(\n",
    "    edit_distance(text1, text2), \n",
    "    edit_distance(text2, text3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6666666666666666"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 02 accuracy 정확도 측정\n",
    "from nltk.metrics import accuracy\n",
    "accuracy('파이썬', '파이프')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "생략된 단어가 다를 때 0.75 \n",
      "어휘 순서를 바꿨을 때 0.08333\n"
     ]
    }
   ],
   "source": [
    "print('생략된 단어가 다를 때 {:.4} \\n어휘 순서를 바꿨을 때 {:.4}'.format(\n",
    "    accuracy(text1, text2), \n",
    "    accuracy(text2, text3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br></br>\n",
    "## **4 Token 고유 객체를 활용한 통계적 검증방법**\n",
    "1. precision = Correct / (Correct + Incorrect + Missing)\n",
    "1. recall = Correct / (Correct + Incorrect + Spurious<가짜>)\n",
    "1. f_measure = (2 X Precision X Recall) / (Precision + Recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12, 12, 12)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text1 = set(text1)\n",
    "text2 = set(text2)\n",
    "text3 = set(text3)\n",
    "len(text1), len(text2), len(text3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.metrics import precision\n",
    "precision({'파이썬'}, {'파르썬'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "생략된 단어가 다를 때 0.75 \n",
      "어휘 순서를 바꿨을 때 0.8333\n"
     ]
    }
   ],
   "source": [
    "print('생략된 단어가 다를 때 {:.4} \\n어휘 순서를 바꿨을 때 {:.4}'.format(\n",
    "    precision(set(text1), set(text2)), \n",
    "    precision(set(text2), set(text3))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "생략된 단어가 다를 때 0.75 \n",
      "어휘 순서를 바꿨을 때 0.8333\n"
     ]
    }
   ],
   "source": [
    "from nltk.metrics import recall\n",
    "print('생략된 단어가 다를 때 {:.4} \\n어휘 순서를 바꿨을 때 {:.4}'.format(\n",
    "    recall(text1, text2), \n",
    "    recall(text2, text3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "생략된 단어가 다를 때 0.75 \n",
      "어휘 순서를 바꿨을 때 0.8333\n"
     ]
    }
   ],
   "source": [
    "from nltk.metrics import f_measure\n",
    "print('생략된 단어가 다를 때 {:.4} \\n어휘 순서를 바꿨을 때 {:.4}'.format(\n",
    "    f_measure(text1, text2), \n",
    "    f_measure(text2, text3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br></br>\n",
    "# **3 N-gram 의 활용**\n",
    "<br></br>\n",
    "## **1 N-gram 생성하기**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 독일 퀘르버 재단 연설문 : 베를린 선언\n",
    "f     = open('./data/베를린선언.txt', 'r')\n",
    "texts_org = f.read()\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "word_token = word_tokenize(texts_org)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('존경하는', '독일', '국민'),\n",
       " ('독일', '국민', '여러분'),\n",
       " ('국민', '여러분', ','),\n",
       " ('여러분', ',', '고국에'),\n",
       " (',', '고국에', '계신')]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.util import ngrams\n",
    "\n",
    "texts_sample = [txt for txt in ngrams(word_token, 3)]\n",
    "texts_sample[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br></br>\n",
    "## **2-1 Point wise Mutual Information**\n",
    "PMI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['존경하는', '독일', '국민', '여러분', '고국에', '계신', '국민', '여러분', '하울젠', '쾨르버재단']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "re_capt = RegexpTokenizer('[가-힣]\\w+')\n",
    "raw_texts = re_capt.tokenize(texts_org)\n",
    "raw_texts[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'존경하는 독일 국민 여러분 고국에 계신 국민 여러분 하울젠 쾨르버재단 이사님과 모드로 동독 총리님을 비롯한 내외 귀빈 여러분 먼저 냉전과 분단을 넘어 통일을 이루고 힘으로 유럽통합과 국제평화를 선도하고 있는 독일과 독일 국민에게 무한한 경의를 표합니다 오늘 자리를 마련해 주신 독일 정부와 쾨르버 재단에도 감사드립니다 아울러 얼마 별세하신 헬무트 총리의 가족'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts = ''\n",
    "for txt in raw_texts:\n",
    "    texts += txt + \" \"\n",
    "texts[:200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/markbaum/Python/python/lib/python3.6/site-packages/konlpy/tag/_okt.py:16: UserWarning: \"Twitter\" has changed to \"Okt\" since KoNLPy v0.4.5.\n",
      "  warn('\"Twitter\" has changed to \"Okt\" since KoNLPy v0.4.5.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 9.92 s, sys: 167 ms, total: 10.1 s\n",
      "Wall time: 3.64 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# 베를린 선언문에 Tag 속성 추가하기\n",
    "from konlpy.tag import Twitter\n",
    "twitter = Twitter()\n",
    "tagged_words = twitter.pos(texts, stem=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br></br>\n",
    "## **2-2 Bi-Gram 을 대상으로 한 PMI**\n",
    "최상위 우도값 10개를 추출한다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<nltk.collocations.BigramCollocationFinder at 0x7fb3409b5d30>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk import collocations\n",
    "\n",
    "finder = collocations.BigramCollocationFinder.from_words(tagged_words)\n",
    "finder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('가능하며', 'Adjective'), ('불가', 'Noun')),\n",
       " (('가리지', 'Verb'), ('않습니다', 'Verb')),\n",
       " (('가스', 'Noun'), ('관', 'Noun')),\n",
       " (('가운데', 'Noun'), ('현재', 'Noun')),\n",
       " (('감사', 'Noun'), ('드립니다', 'Verb')),\n",
       " (('같은', 'Adjective'), ('공감', 'Noun')),\n",
       " (('건너지', 'Verb'), ('않기를', 'Verb')),\n",
       " (('검증', 'Noun'), ('가능하며', 'Adjective')),\n",
       " (('견', 'Noun'), ('지하', 'Noun')),\n",
       " (('겹', 'Noun'), ('치는', 'Verb'))]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "measures = collocations.BigramAssocMeasures()\n",
    "finder.nbest(measures.pmi, 10)   # top 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br></br>\n",
    "## **2-3 Tri-Gram 을 대상으로한 PMI**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<nltk.collocations.TrigramCollocationFinder at 0x7fb32d2079e8>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "finder = collocations.TrigramCollocationFinder.from_words(tagged_words)\n",
    "finder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('가능하며', 'Adjective'), ('불가', 'Noun'), ('역적', 'Noun')),\n",
       " (('가스', 'Noun'), ('관', 'Noun'), ('연결', 'Noun')),\n",
       " (('가운데', 'Noun'), ('현재', 'Noun'), ('생존', 'Noun')),\n",
       " (('같은', 'Adjective'), ('공감', 'Noun'), ('대', 'Suffix')),\n",
       " (('건너지', 'Verb'), ('않기를', 'Verb'), ('바랍니다', 'Verb')),\n",
       " (('검증', 'Noun'), ('가능하며', 'Adjective'), ('불가', 'Noun')),\n",
       " (('견', 'Noun'), ('지하', 'Noun'), ('면서', 'Noun')),\n",
       " (('과도', 'Josa'), ('같은', 'Adjective'), ('공감', 'Noun')),\n",
       " (('닦아', 'Verb'), ('주어', 'Noun'), ('야', 'Josa')),\n",
       " (('마다', 'Josa'), ('흔들리거나', 'Verb'), ('깨져서도', 'Verb'))]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "measures = collocations.TrigramAssocMeasures()\n",
    "finder.nbest(measures.pmi, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br></br>\n",
    "# **4 TF-IDF**\n",
    "Term Frequency-Inverse Document Frequency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br></br>\n",
    "## **1 영문 데이터 전처리**\n",
    "트럼프 취임사 연설문"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Chief', 'Justice', 'Roberts', ',', 'President']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f = open('./data/trump.txt', 'r')\n",
    "texts_org = f.read()\n",
    "f.close()\n",
    "\n",
    "from nltk import word_tokenize\n",
    "texts = word_tokenize(texts_org)\n",
    "texts[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "186"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stopword_eng = stopwords.words('english')\n",
    "\n",
    "import string\n",
    "punct = string.punctuation\n",
    "punct = [punct[i] for i in range(len(punct))]\n",
    "punct = punct + stopword_eng + ['\\n'] \n",
    "len(punct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = [txt.lower()    for txt in texts   if txt.lower() not in punct]\n",
    "document = ''\n",
    "for txt in texts:\n",
    "    document += txt + ' '"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br></br>\n",
    "## **2 tf idf**\n",
    "연설문내 단어들의 빈도를 재조정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip3 install scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.91 ms, sys: 3.82 ms, total: 6.73 ms\n",
      "Wall time: 6.72 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidf_vec   = TfidfVectorizer()\n",
    "transformed = tfidf_vec.fit_transform(raw_documents = [document])\n",
    "index_value = {i[1]:i[0] for i in tfidf_vec.vocabulary_.items()}\n",
    "\n",
    "transformed "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "fully_indexed = []\n",
    "\n",
    "import numpy as np\n",
    "transformed = np.array(transformed.todense())\n",
    "\n",
    "for row in transformed:\n",
    "    fully_indexed.append({index_value[column]:value for (column,value) in enumerate(row)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "america     0.423619\n",
       "american    0.232990\n",
       "people      0.211809\n",
       "country     0.190628\n",
       "nation      0.190628\n",
       "one         0.169447\n",
       "every       0.148266\n",
       "never       0.127086\n",
       "world       0.127086\n",
       "back        0.127086\n",
       "new         0.127086\n",
       "great       0.127086\n",
       "make        0.105905\n",
       "god         0.105905\n",
       "today       0.105905\n",
       "dtype: float64"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "tfidf = pd.Series(fully_indexed[0]).sort_values(ascending=False)\n",
    "tfidf[:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.12708556268223822"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf['great']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
